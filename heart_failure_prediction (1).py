# -*- coding: utf-8 -*-
"""Heart Failure Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SB27wlD5G5S-NtUfu_KTndFjSzhACdbK

**HEART FAILURE PREDICTION**

*Importing libraries*
"""

# Commented out IPython magic to ensure Python compatibility.
#importing the usual libraries
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sb

#importing libraries required for classification 
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import preprocessing

#importing libraries required for model evaluation
from sklearn import metrics

#importing multiple libraries
from sklearn.metrics import (accuracy_score, 
                             precision_score,
                             recall_score,
                             f1_score, 
                             roc_curve,
                             roc_auc_score,
                             confusion_matrix)

#clearing the warnings in Python
from warnings import filterwarnings
filterwarnings('ignore')

#importing Google Drive
from google.colab import drive
drive.mount('drive')

"""*Reading the dataset*"""

#importing pandas library as pd
#reading the data using the pandas library
import pandas as pd
path="/content/heart_failure_clinical_records_dataset.csv"
hs_data=pd.read_csv(path)
hs_data.head(10)

"""*Describing the dataset*"""

#describing the method to get an idea about mean, std for each column
hs_data.describe()

"""*Concise summary of the dataset*"""

#using the info() function to get a concise summary of the data
hs_data.info()

"""*Number of distinct observations*"""

#using the nunique() function to get the number of unique elements in the object
hs_data.nunique()

sb.countplot(x = "sex",  data= hs_data)
plt.xticks(ticks=[0, 1], labels = ["female", "male"])
plt.show()

sb.histplot(x = "age", data=hs_data)
plt.show()

"""*Defining the columns*"""

#defining the columns as categorical and continuous
cat_cols = ['anaemia','diabetes','high_blood_pressure ','sex','smoking','DEATH_EVENT']
con_cols = ["age","creatinine_phosphokinase","ejection_fraction","platelets","serum_creatinine","time"]

"""*Testing and training the dataset*"""

#feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
hs_data[con_cols] = scaler.fit_transform(hs_data[con_cols])

#defining the feature input and the label input
#droping some fields
all_inputs = hs_data.drop(['DEATH_EVENT'], axis=1).values
#extracting the class labels
all_labels = hs_data['DEATH_EVENT'].values
#subset of the inputs looks like:
all_inputs[:100]

all_labels

#spliting the data into training and testing. 
#75% of the data is used for training, while 25% of the data is used for testing.
(x_train,
 x_test,
 y_train,
 y_test) = train_test_split(all_inputs,all_labels , test_size=0.25, random_state=0)

print('Shape for training data', x_train.shape, y_train.shape)
print('Shape for testing data', x_test.shape, y_test.shape)

"""*KNeighborsClassifier*"""

accuracy = []

for i in range(1, 50):   
    classifier = KNeighborsClassifier(n_neighbors = i)
    classifier.fit(x_train, y_train)
    accuracy.append(classifier.score(x_train, y_train))
prediction=classifier.predict(x_test)
print("Accuracy: "+str(classifier.score(x_test, y_test)))
print("F-1 Score: "+str(f1_score(y_test, prediction)))
print("ROC score: "+str(roc_auc_score(y_test, prediction)))
fpr_two,tpr_two,threshold_two=roc_curve(y_test, prediction)
print("ROC_AUC_SCORE: "+str(roc_auc_score(y_test, prediction)))
# print("Confusion matrix:"+str(confusion_matrix(y_test, prediction)))

"""*DecisionTreeClassifier*"""

(x_train,
 x_test,
 y_train,
 y_test) = train_test_split(all_inputs, all_labels, test_size=0.20, random_state=0)

classifier=DecisionTreeClassifier ()
classifier.fit(x_train,y_train)
prediction=classifier.predict(x_test)
print("Accuracy: "+str(classifier.score(x_test, y_test)))
print("F-1 Score: "+str(f1_score(y_test, prediction)))
print("ROC score: "+str(roc_auc_score(y_test, prediction)))

"""*LogisticRegression*"""

(x_train,
 x_test,
 y_train,
 y_test) = train_test_split(all_inputs, all_labels , test_size=0.20, random_state=0)
 
classifier=LogisticRegression()
classifier.fit(x_train,y_train)
prediction=classifier.predict(x_test)
print("Accuracy: "+str(classifier.score(x_test, y_test)))
print("F-1 Score: "+str(f1_score(y_test, prediction)))
print("ROC score: "+str(roc_auc_score(y_test, prediction)))
fpr_one,tpr_one,threshold_one=roc_curve(y_test, prediction)
print("ROC_AUC_SCORE: "+str(roc_auc_score(y_test, prediction)))
# print("Confusion matrix:"+str(confusion_matrix(y_test, prediction)))

"""*RandomForestClassifier*"""

#spliting the data into training and testing. 
#75% of the data is used for training, while 25% of the data is used for testing.
(x_train,
 x_test,
 y_train,
 y_test) = train_test_split(all_inputs, all_labels , test_size=0.20, random_state=0)

classifier = RandomForestClassifier()
classifier.fit(x_train, y_train)
prediction = classifier.predict(x_test)

print("Accuracy: "+str(classifier.score(x_test, y_test)))
print("F-1 Score: "+str(f1_score(y_test, prediction)))
print("ROC score: "+str(roc_auc_score(y_test, prediction)))
fpr_two,tpr_two,threshold_two=roc_curve(y_test, prediction)
print("ROC_AUC_SCORE: "+str(roc_auc_score(y_test, prediction)))
# print("Confusion matrix:"+str(confusion_matrix(y_test, prediction)))

"""*F-1 Score*"""

f1 = f1_score(testing_classes, pred)
print ("F1 Score :", f1)

"""*Comparing dataset*"""

plt.plot(fpr, tpr,color='black',label='KNeighborClassifier')
plt.plot(fpr_one, tpr_one,color='pink',label='LogisticRegression')
plt.plot(fpr_two, tpr_two,color='blue',label='RandomForestClassifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.title('ROC curve for mental health prediction')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.grid(True)
# RandomForestClassifier performs better